# .github/workflows/test.yml
name: Harbor Test Suite

on:
  workflow_call:
    inputs:
      stage:
        description: 'Test stage to run (quality, full-tests)'
        required: false
        default: 'full-tests'
        type: string
    outputs:
      tests-passed:
        description: "Whether all tests passed"
        value: ${{ jobs.test-summary.outputs.tests-passed }}
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

permissions:
  actions: read
  contents: read
  security-events: write
  pull-requests: write
  issues: write

env:
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_PROGRESS_BAR: off

jobs:
  # =============================================================================
  # Stage 1: Code Quality Checks (Fast)
  # =============================================================================
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    if: inputs.stage == 'quality' || inputs.stage == 'full-tests' || github.event_name != 'workflow_call'

    outputs:
      quality-passed: ${{ steps.quality-check.outputs.passed }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for skip indicators
        id: skip-check
        run: |
          # Check commit message for skip indicators
          COMMIT_MSG="${{ github.event.head_commit.message }}"

          if [[ "$COMMIT_MSG" == *"[skip ci]"* ]] || [[ "$COMMIT_MSG" == *"[ci skip]"* ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "reason=commit-message" >> $GITHUB_OUTPUT
            echo "CI skipped due to [skip ci] in commit message"
          elif [[ "$COMMIT_MSG" == *"[skip tests]"* ]]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "reason=skip-tests" >> $GITHUB_OUTPUT
            echo "Tests skipped due to [skip tests] in commit message"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
            echo "reason=" >> $GITHUB_OUTPUT
          fi

      - name: Early exit if skipped
        if: steps.skip-check.outputs.skip == 'true'
        run: |
          echo "Skipping quality checks - Reason: ${{ steps.skip-check.outputs.reason }}"
          echo "quality-passed=true" >> $GITHUB_OUTPUT
          exit 0

      - name: Set up Python
        if: steps.skip-check.outputs.skip != 'true'
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install ruff and other quality tools
          pip install ruff mypy

          # Install project dependencies if they exist
          if [ -f "requirements/development.txt" ]; then
            pip install -r requirements/development.txt
          elif [ -f "requirements/base.txt" ]; then
            pip install -r requirements/base.txt
          elif [ -f "pyproject.toml" ]; then
            pip install -e ".[dev]" || pip install -e "."
          fi

      - name: Run ruff linter
        run: |
          if [ -d "app" ]; then
            echo "Running ruff check on app/ and tests/ directories..."
            # Match pre-commit behavior: use --fix to auto-fix issues
            # But since we can't commit in CI, just check without fix
            ruff check app/ tests/ || true
            # Record the actual check result
            ruff check app/ tests/ --output-format=json > ruff-results.json || echo '{"violations": []}' > ruff-results.json
          else
            echo "No app/ directory found - skipping linting"
            echo '{"violations": []}' > ruff-results.json
          fi

      - name: Run ruff formatter check
        run: |
          if [ -d "app" ]; then
            echo "Running ruff format check..."
            # Just check formatting, don't fail the build
            ruff format --check app/ tests/ || true
          else
            echo "No app/ directory found - skipping format check"
          fi

      - name: Run mypy type checker
        run: |
          if [ -d "app" ]; then
            echo "Running mypy type checker..."
            # Match pre-commit: relaxed settings during M0
            mypy app/ tests/ \
              --ignore-missing-imports \
              --no-strict-optional \
              --allow-untyped-defs || true
          else
            echo "No app/ directory found - skipping type check"
          fi
        continue-on-error: true

      - name: Set quality check result
        id: quality-check
        run: |
          # For M0 development, we're being lenient
          # Quality checks are informational but don't block the build
          echo "passed=true" >> $GITHUB_OUTPUT
          echo "âœ… Quality checks completed (informational only during M0)"

  # =============================================================================
  # Stage 2: Test Suite (Only after quality passes)
  # =============================================================================
  test:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: [quality]  # Wait for quality checks to pass
    if: |
      always() &&
      needs.quality.outputs.quality-passed == 'true' &&
      (inputs.stage == 'full-tests' || github.event_name != 'workflow_call')

    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        python-version: ${{ github.event_name == 'pull_request' && fromJSON('["3.13"]') || fromJSON('["3.11", "3.12", "3.13"]') }}

    outputs:
      tests-passed: ${{ steps.test-result.outputs.passed }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f "requirements/test.txt" ]; then
            pip install -r requirements/test.txt
          elif [ -f "requirements/development.txt" ]; then
            pip install -r requirements/development.txt
          elif [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]"
          else
            pip install pytest pytest-cov pytest-asyncio
          fi

      - name: Create test data directory
        run: mkdir -p data

      - name: Run unit tests
        run: |
          if [ -d "tests/unit" ] && find tests/unit -name "test_*.py" -type f | head -1 > /dev/null; then
            echo "Running existing unit tests..."
            pytest tests/unit/ -v --cov=app --cov-report=xml --cov-report=term-missing --tb=short
          else
            echo "No unit tests found - this is expected during M0 development"
            echo "Creating minimal test structure..."
            mkdir -p tests/unit
            echo 'def test_placeholder(): assert True' > tests/unit/test_placeholder.py
            pytest tests/unit/ -v
          fi
        env:
          HARBOR_MODE: development
          DATABASE_URL: sqlite:///data/test.db

      - name: Run integration tests
        run: |
          if [ -d "tests/integration" ] && find tests/integration -name "test_*.py" -type f | head -1 > /dev/null; then
            pytest tests/integration/ -v
          else
            echo "No integration tests found - skipping"
          fi
        env:
          HARBOR_MODE: development
          DATABASE_URL: sqlite:///data/test_integration.db
        continue-on-error: true

      - name: Set test result
        id: test-result
        run: echo "passed=true" >> $GITHUB_OUTPUT

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.python-version == '3.13' && hashFiles('coverage.xml') != ''
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
        continue-on-error: true

  # =============================================================================
  # Docker Build Test (Only after tests pass)
  # =============================================================================
  docker:
    name: Docker Build Test
    runs-on: ubuntu-latest
    needs: [quality, test]
    if: |
      always() &&
      needs.quality.outputs.quality-passed == 'true' &&
      needs.test.result == 'success'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build development image
        run: |
          docker build -t harbor:test -f deploy/docker/Dockerfile.dev .

      - name: Test Docker image
        run: |
          docker run --rm harbor:test python -c "
          import sys
          print('Docker image builds successfully')
          print(f'Python version: {sys.version}')
          try:
              import fastapi, sqlalchemy, pydantic
              print('Core dependencies available')
          except ImportError as e:
              print(f'Dependency issue: {e}')
          try:
              import app
              print('Harbor app module available')
          except ImportError as e:
              print(f'Harbor app not ready: {e}')
          print('Docker test completed')
          "

  # =============================================================================
  # Documentation Check
  # =============================================================================
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    needs: [quality]
    if: needs.quality.outputs.quality-passed == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check documentation
        run: |
          if [ -f README.md ]; then
            echo "âœ… README.md exists"
            if grep -q "Harbor Container Updater" README.md; then
              echo "âœ… README.md contains project title"
            fi
          else
            echo "âŒ README.md is missing"
          fi
          echo "Documentation check completed"

  # =============================================================================
  # Test Summary (Aggregates all results)
  # =============================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [quality, test, docker, docs]
    if: always()

    outputs:
      tests-passed: ${{ steps.summary.outputs.tests-passed }}

    steps:
      - name: Generate test summary
        id: summary
        run: |
          # Create unique artifact name based on the calling stage
          STAGE="${{ inputs.stage || 'direct' }}"
          TIMESTAMP=$(date +%s)
          UNIQUE_ID="${STAGE}-${TIMESTAMP}-${{ github.run_number }}"

          echo "# ğŸ§ª Harbor Test Summary (${STAGE})" > test-summary-${UNIQUE_ID}.md
          echo "Generated on: $(date -u)" >> test-summary-${UNIQUE_ID}.md
          echo "" >> test-summary-${UNIQUE_ID}.md
          echo "## ğŸ“Š Test Results" >> test-summary-${UNIQUE_ID}.md
          echo "- ğŸ” Code Quality: ${{ needs.quality.result }}" >> test-summary-${UNIQUE_ID}.md
          echo "- ğŸ§ª Test Suite: ${{ needs.test.result }}" >> test-summary-${UNIQUE_ID}.md
          echo "- ğŸ³ Docker Build: ${{ needs.docker.result }}" >> test-summary-${UNIQUE_ID}.md
          echo "- ğŸ“š Documentation: ${{ needs.docs.result }}" >> test-summary-${UNIQUE_ID}.md
          echo "" >> test-summary-${UNIQUE_ID}.md

          # Determine overall result
          if [[ "${{ needs.quality.result }}" == "success" && \
                "${{ needs.test.result }}" == "success" && \
                "${{ needs.docker.result }}" == "success" ]]; then
            echo "tests-passed=true" >> $GITHUB_OUTPUT
            echo "## âœ… Overall Result: PASSED" >> test-summary-${UNIQUE_ID}.md
            echo "" >> test-summary-${UNIQUE_ID}.md
            echo "All tests passed! Ready for Docker build pipeline." >> test-summary-${UNIQUE_ID}.md
          else
            echo "tests-passed=false" >> $GITHUB_OUTPUT
            echo "## âŒ Overall Result: FAILED" >> test-summary-${UNIQUE_ID}.md
            echo "" >> test-summary-${UNIQUE_ID}.md
            echo "Some tests failed. Docker build pipeline will be skipped." >> test-summary-${UNIQUE_ID}.md
          fi

          # Store the filename for upload
          echo "SUMMARY_FILE=test-summary-${UNIQUE_ID}.md" >> $GITHUB_ENV

          cat test-summary-${UNIQUE_ID}.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ${{ env.SUMMARY_FILE }}
          path: ${{ env.SUMMARY_FILE }}
          retention-days: 30

  # =============================================================================
  # PR Test Summary Updating
  # =============================================================================

  pr-comment:
      name: Update PR Test Summary
      needs: [ test-summary ]
      # Fix the condition - ensure we have a valid PR number
      if: |
        github.event_name == 'pull_request' &&
        github.event.pull_request.number != null &&
        always()
      runs-on: ubuntu-latest

      permissions:
        pull-requests: write
        issues: write

      steps:
        - name: Generate test summary
          run: |
            cat > test-summary.md << 'EOF'
            <!-- harbor-pr-comment:test-summary -->
            ## ğŸ§ª Harbor Test Results

            ### Test Suite Status

            | Component | Status |
            |-----------|--------|
            | ğŸ” Code Quality | ${{ needs.quality.result }} |
            | ğŸ§ª Unit Tests | ${{ needs.test.result }} |
            | ğŸ³ Docker Build | ${{ needs.docker.result }} |
            | ğŸ“š Documentation | ${{ needs.docs.result }} |

            **Overall Result**: ${{ needs.test-summary.outputs.tests-passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}

            ### Details
            - Python versions tested: 3.11, 3.12, 3.13
            - Test coverage: View in artifacts

            [View Full Test Report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ---
            _Updated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')_
            EOF

        - name: Find existing comment
          uses: peter-evans/find-comment@v2
          id: find-comment
          with:
            issue-number: ${{ github.event.pull_request.number }}
            comment-author: 'github-actions[bot]'
            body-includes: '<!-- harbor-pr-comment:test-summary -->'

        - name: Create or update comment
          uses: peter-evans/create-or-update-comment@v3
          with:
            comment-id: ${{ steps.find-comment.outputs.comment-id }}
            issue-number: ${{ github.event.pull_request.number }}
            body-path: test-summary.md
            edit-mode: replace

  # =============================================================================
  # Deployment Readiness Check (Only on main branch)
  # =============================================================================
  deploy-check:
    name: Deployment Readiness
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: |
      always() &&
      github.ref == 'refs/heads/main' &&
      needs.test-summary.outputs.tests-passed == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Check version consistency
        run: |
          echo "ğŸ“¦ Checking project version consistency..."

          # Check pyproject.toml version using simple grep
          if [ -f "pyproject.toml" ]; then
            echo "Checking pyproject.toml..."
            PROJ_VERSION=$(grep '^version = ' pyproject.toml | sed 's/version = "\(.*\)"/\1/' || echo "unknown")
            echo "Project version: $PROJ_VERSION"
          else
            echo "No pyproject.toml found"
          fi

          # Check app/__init__.py version
          if [ -f "app/__init__.py" ]; then
            echo "Checking app/__init__.py..."
            APP_VERSION=$(grep '__version__ = ' app/__init__.py | sed 's/__version__ = "\(.*\)"/\1/' || echo "unknown")
            echo "App version: $APP_VERSION"

            MILESTONE=$(grep '__milestone__ = ' app/__init__.py | sed 's/__milestone__ = "\(.*\)"/\1/' || echo "unknown")
            echo "App milestone: $MILESTONE"
          else
            echo "No app/__init__.py found"
          fi

          echo "âœ… Version check completed (detailed validation happens in release process)"


      - name: Deployment readiness summary
        run: |
          echo "ğŸ‰ All checks passed - ready for deployment"
          echo "- Code quality: âœ… PASSED"
          echo "- Tests: âœ… PASSED"
          echo "- Docker build: âœ… PASSED"
          echo "- Documentation: âœ… PASSED"
          echo ""
          echo "ğŸ“¦ Docker build pipeline can proceed"
          echo "ğŸ”’ Security scanning will follow"
          echo ""
          echo "Current Status: M0 Foundation Phase"
          echo "Next Milestone: M1 Container Discovery"
